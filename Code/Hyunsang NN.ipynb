{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change working directory \n",
    "os.chdir('C:/Users/ehyun/Documents/Kaggle/NeurIPS 2024/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in parquet file as dataframe\n",
    "df = pd.read_parquet('features_withoutdupes.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fp_0</th>\n",
       "      <th>fp_1</th>\n",
       "      <th>fp_2</th>\n",
       "      <th>fp_3</th>\n",
       "      <th>fp_4</th>\n",
       "      <th>fp_5</th>\n",
       "      <th>fp_6</th>\n",
       "      <th>fp_7</th>\n",
       "      <th>fp_8</th>\n",
       "      <th>fp_9</th>\n",
       "      <th>...</th>\n",
       "      <th>fp_162</th>\n",
       "      <th>fp_163</th>\n",
       "      <th>fp_164</th>\n",
       "      <th>fp_165</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>MolLogP</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>binds_BRD4</th>\n",
       "      <th>binds_HSA</th>\n",
       "      <th>binds_sEH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>573.411</td>\n",
       "      <td>1.50200</td>\n",
       "      <td>3574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>637.045</td>\n",
       "      <td>3.21830</td>\n",
       "      <td>4715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>621.072</td>\n",
       "      <td>2.10180</td>\n",
       "      <td>7581</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>664.162</td>\n",
       "      <td>2.53100</td>\n",
       "      <td>7605</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>667.118</td>\n",
       "      <td>2.62782</td>\n",
       "      <td>7613</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fp_0  fp_1  fp_2  fp_3  fp_4  fp_5  fp_6  fp_7  fp_8  fp_9  ...  fp_162  \\\n",
       "0     0     0     0     0     0     1     0     0     0     0  ...       1   \n",
       "2     0     0     0     0     0     1     0     0     0     0  ...       1   \n",
       "3     0     0     0     0     0     1     0     0     0     0  ...       1   \n",
       "4     0     0     0     0     0     1     0     0     0     0  ...       1   \n",
       "5     0     0     0     0     0     1     0     0     0     0  ...       1   \n",
       "\n",
       "   fp_163  fp_164  fp_165    MolWt  MolLogP  Unnamed: 0  binds_BRD4  \\\n",
       "0       1       1       0  573.411  1.50200        3574           1   \n",
       "2       1       1       0  637.045  3.21830        4715           1   \n",
       "3       1       1       0  621.072  2.10180        7581           1   \n",
       "4       1       1       0  664.162  2.53100        7605           1   \n",
       "5       1       1       0  667.118  2.62782        7613           1   \n",
       "\n",
       "   binds_HSA  binds_sEH  \n",
       "0          0          0  \n",
       "2          0          0  \n",
       "3          0          0  \n",
       "4          0          0  \n",
       "5          0          0  \n",
       "\n",
       "[5 rows x 172 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting list of column names to create X and Y variables \n",
    "columns = list(df.columns)\n",
    "features = df[columns[:-4]]\n",
    "outputs = df[columns[-3:]] # ignore the index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input data (features) as a tensor\n",
    "X = torch.tensor(features.values)\n",
    "# Get output data (outputs) as a tensor\n",
    "y = torch.tensor(outputs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   1.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   1.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   1.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   1.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   1.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   1.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   1.0000,   0.0000,   0.0000,   0.0000,   1.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   1.0000,   1.0000,\n",
      "          0.0000,   0.0000,   1.0000,   1.0000,   1.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   1.0000,   0.0000,   0.0000,   1.0000,\n",
      "          1.0000,   0.0000,   1.0000,   0.0000,   0.0000,   1.0000,   1.0000,\n",
      "          1.0000,   1.0000,   0.0000,   0.0000,   1.0000,   1.0000,   0.0000,\n",
      "          1.0000,   1.0000,   1.0000,   0.0000,   1.0000,   1.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   1.0000,   1.0000,   1.0000,   0.0000,\n",
      "          1.0000,   1.0000,   0.0000,   0.0000,   1.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   1.0000,   1.0000,   1.0000,\n",
      "          1.0000,   1.0000,   1.0000,   1.0000,   1.0000,   0.0000,   0.0000,\n",
      "          0.0000,   1.0000,   0.0000,   0.0000,   0.0000,   0.0000,   1.0000,\n",
      "          0.0000,   0.0000,   0.0000,   1.0000,   0.0000,   1.0000,   1.0000,\n",
      "          1.0000,   1.0000,   0.0000,   1.0000,   1.0000,   1.0000,   1.0000,\n",
      "          1.0000,   1.0000,   1.0000,   1.0000,   0.0000, 573.4110,   1.5020],\n",
      "       dtype=torch.float64)\n",
      "tensor([1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# print first item in each tensor to see if conversion worked\n",
    "print(X.data[0])\n",
    "print(y.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training (80%) and test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2,\n",
    "                                                    random_state = 55) # keep the same random splitting each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2024127, 168])\n",
      "torch.Size([2024127, 3])\n",
      "torch.Size([506032, 168])\n",
      "torch.Size([506032, 3])\n"
     ]
    }
   ],
   "source": [
    "# converting the tensors so that they contain float values\n",
    "X_train = X_train.float()\n",
    "y_train = y_train.float()\n",
    "X_test = X_test.float()\n",
    "y_test = y_test.float()\n",
    "\n",
    "\n",
    "print(X_train.size())\n",
    "print(y_train.size())\n",
    "print(X_test.size())\n",
    "print(y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([292560, 281858, 281350])\n",
      "tensor([73309, 70635, 70883])\n"
     ]
    }
   ],
   "source": [
    "# seeing the total number of hits for each protein in train / test set\n",
    "train_sums = torch.sum(y_train, dim = 0)\n",
    "test_sums = torch.sum(y_test, dim = 0)\n",
    "print(train_sums)\n",
    "print(test_sums)\n",
    "# BRD4, HSA, sEH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feedforward neural network\n",
    "\n",
    "# Defining the class of the model\n",
    "class DrugTargets(nn.Module): # reference nn.Module from 'nn' import\n",
    "    def __init__(self):\n",
    "        super(DrugTargets, self).__init__() # super gives access to methods from the nn.Module class\n",
    "\n",
    "        # Create the first layer, taking in all inputs (168)\n",
    "        self.fc1 = nn.Linear(168, 300) # fc = fully connected, 168 = inputs, 300 = outputs\n",
    "        self.relu = nn.ReLU() # ReLU activation function applied to the inputs\n",
    "        \n",
    "        # Create the hidden layers\n",
    "        self.fc2 = nn.Linear(300, 150) # Hidden layer 1 \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(150, 3) # Hidden layer 2, final output must be for 3 targets\n",
    "        self.sigmoid = nn.Sigmoid() # Final activation function is sigmoid to output probability for each drug target\n",
    "    \n",
    "    def forward(self, x): # Tells the model how to produce the output tensor when an input tensor is given\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = DrugTargets()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.BCELoss() # binary cross entropy\n",
    "opt = optim.Adam(model.parameters(), lr = 0.0001) # lr = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     12\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 13\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, latest loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ehyun\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ehyun\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\ehyun\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     adam(\n\u001b[0;32m    169\u001b[0m         params_with_grad,\n\u001b[0;32m    170\u001b[0m         grads,\n\u001b[0;32m    171\u001b[0m         exp_avgs,\n\u001b[0;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    174\u001b[0m         state_steps,\n\u001b[0;32m    175\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    176\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    177\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    178\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    179\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    182\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    183\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    184\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    186\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    187\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    189\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\ehyun\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m func(params,\n\u001b[0;32m    319\u001b[0m      grads,\n\u001b[0;32m    320\u001b[0m      exp_avgs,\n\u001b[0;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    323\u001b[0m      state_steps,\n\u001b[0;32m    324\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    325\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    326\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    327\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    328\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    329\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    330\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    331\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    332\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    333\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    334\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    335\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\ehyun\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:394\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    393\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    397\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop to give batches of data to the model and set number of epochs (rounds of training)\n",
    "\n",
    "####### CANCELLED THIS CODE BECAUSE IT DIDNT RUN IN TIME ########### \n",
    "####### TOO LOW OF BATCH SIZE? ##########\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X_train[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y_train[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OUTPUTS FROM HERE ARE WITH THE MODEL BEING TRAINED WITH 10 EPOCHS, 1,000,000 BATCH SIZE ####\n",
    "#### BAD RESULTS#####\n",
    "\n",
    "# predicting the test set \n",
    "y_preds = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8585)\n"
     ]
    }
   ],
   "source": [
    "accuracy = (y_preds.round() == y_test).float().mean()\n",
    "# The round() function rounds off the floating point to the nearest integer. \n",
    "# The == operator compares and returns a Boolean tensor, which can be converted to floating point numbers 1.0 and 0.0. \n",
    "# The mean() function will provide you the count of the number of 1â€™s \n",
    "# (i.e., prediction matches the label) divided by the total number of samples.\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0916, 0.0619, 0.1084])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds.data[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for target 1: 0.0\n",
      "Precision for target 2: 0.0\n",
      "Precision for target 3: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Threshold the predictions\n",
    "threshold = 0.5\n",
    "y_preds_binary = (y_preds >= threshold).float()\n",
    "\n",
    "# Calculate True Positives (TP) and False Positives (FP)\n",
    "TP = torch.sum((y_preds_binary == 1) & (y_test == 1), dim=0)\n",
    "FP = torch.sum((y_preds_binary == 1) & (y_test == 0), dim=0)\n",
    "\n",
    "# Compute precision for each target\n",
    "precision = TP / (TP + FP + 1e-10)  # Adding a small epsilon to avoid division by zero\n",
    "\n",
    "# Print precision for each target\n",
    "for i, prec in enumerate(precision):\n",
    "    print(f'Precision for target {i+1}: {prec.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1 predictions for target 1: 0.0\n",
      "Number of 1 predictions for target 2: 0.0\n",
      "Number of 1 predictions for target 3: 0.0\n"
     ]
    }
   ],
   "source": [
    "num_ones_per_target = torch.sum(y_preds_binary, dim=0)\n",
    "\n",
    "# Print the number of 1 predictions for each target\n",
    "for i, num_ones in enumerate(num_ones_per_target):\n",
    "    print(f'Number of 1 predictions for target {i+1}: {num_ones.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
